{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook  \n",
    "\n",
    "TBD...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = \"./\"\n",
    "MODEL_DIR = \"../input/cassava-model/\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "TRAIN_PATH = \"../input/cassava-leaf-disease-classification/train_images\"\n",
    "TEST_PATH = \"../input/cassava-leaf-disease-merged/train\"\n",
    "# TEST_PATH = \"../input/cassava-leaf-disease-classification/train_images\"\n",
    "# TEST_PATH = \"../input/cassava-leaf-disease-classification/test_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    debug = False\n",
    "    num_workers = 4\n",
    "    models = [\n",
    "        # \"tf_efficientnet_b3_ns\",\n",
    "        \"tf_efficientnet_b4_ns\",\n",
    "        \"vit_base_patch16_384\",\n",
    "        # \"deit_base_patch16_384\",\n",
    "        \"seresnext50_32x4d\",\n",
    "    ]\n",
    "    size = {\n",
    "        \"tf_efficientnet_b3_ns\": 512,\n",
    "        \"tf_efficientnet_b4_ns\": 512,\n",
    "        \"vit_base_patch16_384\": 384,\n",
    "        \"deit_base_patch16_384\": 384,\n",
    "        \"seresnext50_32x4d\": 512,\n",
    "    }\n",
    "    batch_size = 64\n",
    "    seed = 22\n",
    "    target_size = 5\n",
    "    target_col = \"label\"\n",
    "    n_fold = 5\n",
    "    trn_fold = {  # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        \"tf_efficientnet_b3_ns\": {\n",
    "            \"best\": [0, 1, 2, 3, 4],\n",
    "            \"final\": [],\n",
    "        },\n",
    "        \"tf_efficientnet_b4_ns\": {\n",
    "            \"best\": [0, 1, 2, 3, 4],\n",
    "            \"final\": [],\n",
    "        },\n",
    "        \"vit_base_patch16_384\": {\"best\": [0, 1, 2, 3, 4], \"final\": []},\n",
    "        \"deit_base_patch16_384\": {\"best\": [0, 1, 2, 3, 4], \"final\": []},\n",
    "        \"seresnext50_32x4d\": {\"best\": [5, 6, 7, 8, 9], \"final\": []},\n",
    "    }\n",
    "    data_parallel = {\n",
    "        \"tf_efficientnet_b3_ns\": False,\n",
    "        \"tf_efficientnet_b4_ns\": True,\n",
    "        \"vit_base_patch16_384\": False,\n",
    "        \"deit_base_patch16_384\": False,\n",
    "        \"seresnext50_32x4d\": False,\n",
    "    }\n",
    "    weight = {\n",
    "        # \"tf_efficientnet_b3_ns\": None,\n",
    "        \"tf_efficientnet_b4_ns\": 1,\n",
    "        \"vit_base_patch16_384\": 1,\n",
    "        # \"deit_base_patch16_384\": None,\n",
    "        \"seresnext50_32x4d\": 1,\n",
    "    }\n",
    "    tta = 3  # 1: no TTA, >1: TTA\n",
    "    train = False\n",
    "    inference = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_sum = sum([CFG.weight[model] for model in CFG.models]) * CFG.tta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../input/pytorch-image-models/pytorch-image-models-master\")\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from contextlib import contextmanager\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from albumentations import (\n",
    "    CenterCrop,\n",
    "    CoarseDropout,\n",
    "    Compose,\n",
    "    Cutout,\n",
    "    HorizontalFlip,\n",
    "    HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise,\n",
    "    ImageOnlyTransform,\n",
    "    Normalize,\n",
    "    OneOf,\n",
    "    RandomBrightness,\n",
    "    RandomBrightnessContrast,\n",
    "    RandomContrast,\n",
    "    RandomCrop,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    Rotate,\n",
    "    ShiftScaleRotate,\n",
    "    Transpose,\n",
    "    VerticalFlip,\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    LOGGER.info(f\"[{name}] start\")\n",
    "    yield\n",
    "    LOGGER.info(f\"[{name}] done in {time.time() - t0:.0f} s.\")\n",
    "\n",
    "\n",
    "def init_logger(log_file=OUTPUT_DIR + \"inference.log\"):\n",
    "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label  source\n",
       "0  1000015157.jpg      0    2020\n",
       "1  1000201771.jpg      3    2020\n",
       "2   100042118.jpg      1    2020\n",
       "3  1000723321.jpg      1    2020\n",
       "4  1000812911.jpg      3    2020"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test = pd.read_csv(\"../input/cassava-leaf-disease-classification/sample_submission.csv\")\n",
    "# test = pd.read_csv(\"../input/cassava-leaf-disease-classification/train.csv\")\n",
    "test = pd.read_csv(\"../input/cassava-leaf-disease-merged/merged.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df[\"image_id\"].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f\"{TEST_PATH}/{file_name}\"\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Transforms\n",
    "# ====================================================\n",
    "def get_transforms(*, data, size):\n",
    "\n",
    "    if data == \"train\":\n",
    "        return Compose(\n",
    "            [\n",
    "                # Resize(size, size),\n",
    "                RandomResizedCrop(size, size),\n",
    "                Transpose(p=0.5),\n",
    "                HorizontalFlip(p=0.5),\n",
    "                VerticalFlip(p=0.5),\n",
    "                ShiftScaleRotate(p=0.5),\n",
    "                HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "                RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "                CoarseDropout(p=0.5),\n",
    "                Cutout(p=0.5),\n",
    "                Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225],\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if data == \"valid\":\n",
    "        return Compose(\n",
    "            [\n",
    "                Resize(size, size),\n",
    "                CenterCrop(size, size),\n",
    "                Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225],\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if data == \"inference\":\n",
    "        return Compose(\n",
    "            [\n",
    "                # Resize(size, size),\n",
    "                RandomResizedCrop(size, size),\n",
    "                Transpose(p=0.5),\n",
    "                HorizontalFlip(p=0.5),\n",
    "                VerticalFlip(p=0.5),\n",
    "                # ShiftScaleRotate(p=0.5),\n",
    "                # HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "                # RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "                # CoarseDropout(p=0.5),\n",
    "                # Cutout(p=0.5),\n",
    "                Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225],\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# MODEL\n",
    "# ====================================================\n",
    "class CassvaImgClassifier(nn.Module):\n",
    "    def __init__(self, model_name=\"resnext50_32x4d\", pretrained=False):\n",
    "        super().__init__()\n",
    "\n",
    "        if model_name == \"deit_base_patch16_384\":\n",
    "            # self.model = torch.hub.load(\"facebookresearch/deit:main\", model_name, pretrained=pretrained)\n",
    "            self.model = torch.hub.load(\"../input/fair-deit\", model_name, pretrained=pretrained, source=\"local\")\n",
    "            n_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(n_features, CFG.target_size)\n",
    "\n",
    "        else:\n",
    "            self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "\n",
    "            if \"resnext50_32x4d\" in model_name:\n",
    "                n_features = self.model.fc.in_features\n",
    "                self.model.fc = nn.Linear(n_features, CFG.target_size)\n",
    "\n",
    "            elif model_name.startswith(\"tf_efficientnet\"):\n",
    "                n_features = self.model.classifier.in_features\n",
    "                self.model.classifier = nn.Linear(n_features, CFG.target_size)\n",
    "\n",
    "            elif model_name.startswith(\"vit_\"):\n",
    "                n_features = self.model.head.in_features\n",
    "                self.model.head = nn.Linear(n_features, CFG.target_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "def inference(model, states, test_loader, device, data_parallel):\n",
    "    model.to(device)\n",
    "\n",
    "    # Use multi GPU\n",
    "    if device == torch.device(\"cuda\") and data_parallel:\n",
    "        model = torch.nn.DataParallel(model)  # make parallel\n",
    "        # torch.backends.cudnn.benchmark=True\n",
    "\n",
    "    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    probs = []\n",
    "    for i, (images) in tk0:\n",
    "        images = images.to(device)\n",
    "        avg_preds = []\n",
    "        for state in states:\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(images)\n",
    "            avg_preds.append(y_preds.softmax(1).to(\"cpu\").numpy())\n",
    "        avg_preds = np.mean(avg_preds, axis=0)\n",
    "        probs.append(avg_preds)\n",
    "    probs = np.concatenate(probs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== Model: tf_efficientnet_b4_ns, TTA: 0, Saved: best, Fold: [0, 1, 2, 3, 4] ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6086611b0e024beb8bbb045cea5e4ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference example: [0.4113303  0.16465071 0.19031808 0.01710353 0.21659732]\n",
      "========== Model: tf_efficientnet_b4_ns, TTA: 1, Saved: best, Fold: [0, 1, 2, 3, 4] ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc7837609464fbab02805c9a0e4acc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference example: [0.48532075 0.15037784 0.23644516 0.02698353 0.10087272]\n",
      "========== Model: tf_efficientnet_b4_ns, TTA: 2, Saved: best, Fold: [0, 1, 2, 3, 4] ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244203c0a32b4830b958983d6ded1a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference example: [0.3294753  0.21423021 0.29620296 0.02863643 0.1314551 ]\n",
      "========== Model: vit_base_patch16_384, TTA: 0, Saved: best, Fold: [0, 1, 2, 3, 4] ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbca783748742c5a57ff105fe3ee972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference example: [0.44151726 0.12938017 0.24077013 0.04045077 0.14788169]\n",
      "========== Model: vit_base_patch16_384, TTA: 1, Saved: best, Fold: [0, 1, 2, 3, 4] ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1dfe49ec98245afba5bd2a0bf9694bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference example: [0.42230922 0.08196902 0.32993802 0.05142972 0.11435406]\n",
      "========== Model: vit_base_patch16_384, TTA: 2, Saved: best, Fold: [0, 1, 2, 3, 4] ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64f7cfad45b4d9885f33f23454ac17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference example: [0.32653087 0.31568345 0.15996166 0.05122393 0.14660004]\n",
      "========== Model: seresnext50_32x4d, TTA: 0, Saved: best, Fold: [5, 6, 7, 8, 9] ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbb463cc7b245bba7f9b233fca4c78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference example: [0.2990355  0.3897496  0.09844354 0.02557188 0.1871995 ]\n",
      "========== Model: seresnext50_32x4d, TTA: 1, Saved: best, Fold: [5, 6, 7, 8, 9] ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5e14499b064731934105bc61462073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference example: [0.26858085 0.20290053 0.23426536 0.03780187 0.2564514 ]\n",
      "========== Model: seresnext50_32x4d, TTA: 2, Saved: best, Fold: [5, 6, 7, 8, 9] ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf74abe5f034a3cb6aeeb1b64b5c6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference example: [0.2038373  0.23594086 0.16766731 0.07201287 0.32054168]\n",
      "========== Overall ==========\n",
      "Submission example: [0.35421526 0.20943135 0.21711247 0.03902384 0.18021706]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label  source\n",
       "0  1000015157.jpg      0    2020\n",
       "1  1000201771.jpg      3    2020\n",
       "2   100042118.jpg      4    2020\n",
       "3  1000723321.jpg      1    2020\n",
       "4  1000812911.jpg      3    2020"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# inference\n",
    "# ====================================================\n",
    "predictions = None\n",
    "for model_name in CFG.models:\n",
    "    for i in range(CFG.tta):\n",
    "        model = CassvaImgClassifier(model_name, pretrained=False)\n",
    "        states = []\n",
    "        for saved_model in [\"best\", \"final\"]:\n",
    "            if CFG.trn_fold[model_name][saved_model] != []:\n",
    "                LOGGER.info(\n",
    "                    f\"========== Model: {model_name}, TTA: {i}, Saved: {saved_model}, Fold: {CFG.trn_fold[model_name][saved_model]} ==========\"\n",
    "                )\n",
    "                states += [\n",
    "                    torch.load(MODEL_DIR + f\"{model_name}_fold{fold}_{saved_model}.pth\")\n",
    "                    for fold in CFG.trn_fold[model_name][saved_model]\n",
    "                ]\n",
    "\n",
    "        if i == 0:  # no TTA\n",
    "            test_dataset = TestDataset(test, transform=get_transforms(data=\"valid\", size=CFG.size[model_name]))\n",
    "        else:\n",
    "            test_dataset = TestDataset(test, transform=get_transforms(data=\"inference\", size=CFG.size[model_name]))\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True\n",
    "        )\n",
    "\n",
    "        inf = inference(model, states, test_loader, device, CFG.data_parallel[model_name])\n",
    "\n",
    "        LOGGER.info(f\"Inference example: {inf[0]}\")\n",
    "\n",
    "        if predictions is None:\n",
    "            predictions = inf[np.newaxis] * CFG.weight[model_name]\n",
    "        else:\n",
    "            predictions = np.append(predictions, inf[np.newaxis] * CFG.weight[model_name], axis=0)\n",
    "\n",
    "sub = np.sum(predictions, axis=0) / weight_sum\n",
    "LOGGER.info(f\"========== Overall ==========\")\n",
    "LOGGER.info(f\"Submission example: {sub[0]}\")\n",
    "np.savetxt(f\"{OUTPUT_DIR}/predictions.csv\", sub, delimiter=\",\")\n",
    "\n",
    "# submission\n",
    "test[\"label\"] = sub.argmax(1)\n",
    "test[[\"image_id\", \"label\"]].to_csv(OUTPUT_DIR + \"submission.csv\", index=False)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv(\"../input/cassava-leaf-disease-classification/train.csv\")\n",
    "train = pd.read_csv(\"../input/cassava-leaf-disease-merged/merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(train[\"label\"], test[\"label\"])\n",
    "cm = cm / cm.astype(np.float).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD4CAYAAABPLjVeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAA7SUlEQVR4nO3deVxU1fvA8c9hcRdwY1DBSrFc+5a5tCmCsimyuC9ZmWb5VUuzXNPKb+77kpWmbbZp7oKiAq6lZptLZam5gDFjKqVWAsP5/TGIDKBAwsw4v+ft675ezNwzd57He+bhcO6de5XWGiGEELbhYu8AhBDi/xMpukIIYUNSdIUQwoak6AohhA1J0RVCCBtyK+038H5qhdOdHvHrW93sHUKpcFHK3iGUOCdMCYB/Msz2DqFUeJV3veU9Vv7+IUWuOX9/u9DmPURGukIIYUOlPtIVQgibUo49lpSiK4RwLi6u9o7gpqToCiGci4NP5EvRFUI4F5leEEIIG5KRrhBC2JCMdIUQwoZkpCuEEDYkZy8IIYQNyfSCEELYkEwvCCGEDclIVwghbEiKrhBC2JCrHEgTQgjbkTldIYSwIZleEEIIG3Lwka5j/0q4gcAmPnwxOYx9U8IZ2qFBvvW1q1Zg9UttSXglmO2vhdCuqY8doizYnt27iOkURmSHEN59Z3G+9enp6Yx6cTiRHUJ4vHd3zqYkA7D3iz307t6Z7jGd6N29M/v37c15zcL5cwhv35ZHWjazWR657dm9k6iIUDqFB7PsBjmNHDGMTuHBPNarGynZOaWlXWRAv7481OJ+pkyaaPWaBfPmENougIda3G+THK7Zs2snkR1DiQgLZumSgnN5acQwIsKC6dPzei4AS5e8TURYMJEdQ9mze5fV68xmM927RDPkv8/kPPfJR8uJCAvmP43v4eLFC6WXVB5f7tlFt6gOdOkUyvvLluRbn56ezriRL9ClUyhPPdaDsykpVutTfztL24ceYPn7y6yeN5vN9O3RmReGDirV+AulXIq+2MFtV3RdlGLaY83oNWcXj74cT+dWdbi7lodVm+GdGrL+qzO0e20rA9/ey7S+D9gpWmtms5lpkyayYNESVq3byOZNsZw4fsyqzdrVn+Ph4cH6uC306fsE8+bMAsCrShXmLXyTFWs2MHHSVMaPHZnzmjYBgXzwyQqb5nKN2WxmyusTeePNd1i9PpbNcRs5nienNatX4uHhwYZNW3ms75PMmz0TgLJlyjJ46PO88OLIfNsNaBvI8k9X2iSHa8xmM5MnTWTRW++w5loux/LkssqSy8bNW3ns8SeZm53L8WPH2BwXy+r1sSx6+x0mv/4aZvP1W+p89OEH1K1bz2pb9zVrxttL36VWrdqln1w2s9nMjCmvM/eNt/l09Qa2bI7L1wfXr1lFZQ8PVm2Ip+djT/DGvFlW6+fOms5Dj7TOt+3PPv6QO++ql+95m1Oq6IsdFFp0lVINlFKjlFLzs5dRSqmGtgiuIM3qVuVX02VOnbtChjmLNftOE3ZfLetGGiqXdwfAo7w7xrS/7RBpfocPHcS3Th18/fxwdy9DaHgHticlWLXZnpRARGQ0AO2CQ/lq35dorWnQsBE1vA0A1POvz9V/rpKeng7Avf+5jxo1vG2ayzWHDx3Er84duXLqyPbEPDklJtIpKgaA9iGh7M/OqXyFCtzfrDllypbNt1175HT40EH8/LJzKVOGsA4d8+2fpMREIrNzCQ4JZf9eSy7bkxII69CRMmXK4Ovrh5/fHRw+dBAAY2oqu3ZuJ6ZLV6ttNWzYiNq1fW2TXLYfDh/C168OtX0t+ys4NJyd2xOt2uzcnkjHTtEABLUP4av9e9HactuxHYnbqFWrNnXr+Vu9xmhMZc+uHUR17mKTPG7KxbXoiz3Cu9lKpdQo4FNAAfuzFwV8opQaXfrh5efjVZ6UC3/lPP7t4t/UrFLeqs30dUfo8lAdvpsZwcfDWjPmo29tHWaBzpmM+PjUzHnsbfDBZDTmaWPKaePm5kalSpVJS0uzapOwNZ4GDRtRpkyZUo+5MCaTER+f69M3BoMBk8lYQJu8OV20aZxFYTIa8al5PRdvgwGjsZBcKltyMRqNGHL/P/gYcvbt9KmTGT7iJVxc7P+HpclkHae3wYdzJpNVm3MmI97Zba7trz/S0vjrryt88N5SBjz733zbnTNjKkOGvYhyhINYt/n0Qn+ghdZ6qtZ6efYyFWiZva5ASqmBSqkDSqkDfx/dVpLxFknnVnX4bM9J7ntxI73n7uKNp1s6+tx6kR0/9gvz58xi3Cuv2TsUUQQ7tidRtWpVGjVuYu9QbtmSt96gV5/HqVChotXzu3dup2qVqjRs1NhOkeXh4NMLhZ29kAXUAk7leb5m9roCaa0XA4uh5G/Bnpr2N7WrVrgeSJXy/HbRevqgd+u76Dl7JwAHjp+nnLsr1SqV5fdLV0sylGKr4W0gNfW3nMcmYyreBkOeNt6kpv6GwceHzMxMLl++hJeXF2D5M3XEsCFMnDwNP786tgz9hry9DaSmpuY8NhqNeHsbCmiTN6cqtg61UN4GA6m/Xc/FZDRiMBSSyyVLLgaDAWPu/4dUI94GA9uTEtm+PZHdu3Zy9epVrly5zJhRLzJl2kyb5ZU3/txxmoyp1PC2nsap4W3AlJqKwXB9f3l6eXHk0EGStm5h4dxZXLp0CRcXRdmyZTGZjOzckcQXu3dyNf0qV65c4ZWxI3lt8nRbp2fhCKPtmygsumFAglJqk1JqcfayGUgAni/16Arw7a8XqGuoRJ3qFXF3dSGmVR3ivztr1Sblwl+0bmT5sNSvWZmy7q52L7gAjZs05cypU6QkJ5ORkU78pjgC2gZZtQloG8TG9WsByzRCi5YPopTi0p9/8tzgZxg6bAT33W+fsxQK0rhJU06fPklK8pnsnGIJCMyTU2AQG9atAWDblnhatLLk5Giu5ZKcfIaM9HQ2x+XPpW1gEOuzc9m6JZ6W2bkEBAaxOS6W9PR0kpPPcPr0SZo0vZfnh49ga+JONm1NZNrM2bRo9aDdCi5Aw8ZNOHP6FGdTLH1wa/wm2gQEWrVpHRBI7Ia1ACRu20LzFq1QSrH43eWs3bSNtZu20bNPX57oP5BuPfsw+LkX2LglibWbtvH61Fk0b9HKfgUXHH564aYjXa31ZqXU3VimE64dYk0BvtJam2/8ytJjztKMXv4Nn73QBlcXxce7f+Xo2T8ZFd2Y705eJP67s7zy2ffMfqI5z4bcjdaa55but0eo+bi5uTFq7HgGP9ufLHMWkTFdqOdfnzcXzqdR4yYEBAYR3bkr48eMJLJDCJ6enkyZPhuAzz75iDNnTrPkrUUseWsRAIveXkrVatWYO3sGm2M38s8/fxPWLoDoLl159r9DbZbT6LETGPTMALLMZqJiuuDvX59FC+fRqHET2ga2I6ZzV8aNeYlO4cF4eHoybcacnNeHhwRx5fJlMjIySErcxpuLl1Gvnj9zZk1nU5wlp5B2bYjp3I1Bg0s3Jzc3N8aMm8CggQPIyjITnZ3LGwvm0bhxE9oGtSOmS1fGjX6JiDBLLtNnWnLx969PSFg4MZEdcHV1ZezLE3At5OuoHy3/gPeWvcP533+nW0wkj7YJ4NWJk0o9xxdHj+O5QU+TlZVFp6gY6vrX5+1FC2jYqDFt2gYRGdOFV8eNokunUDw8vHjdjr8k/hUHv56uunZUsrSU9PSCI/j1rW72DqFUuDjg6PNWOWFKAPyTYZcxT6nzKu96y3usfPTiItecv9cOtHkPkW+kCSGci4PP6UrRFUI4Fwf/80aKrhDCqTjiQdrcpOgKIZyKFF0hhLAh5SJFVwghbEZGukIIYUNSdIUQwoak6AohhC05ds2VoiuEcC4y0hVCCBtyhOsW34xjRyeEEMWklCryUoRthSmljiqljhV04walVB2lVJJS6lul1EGlVIfCtilFVwjhXFQxlpttRilX4A0gHGgE9FJKNcrT7GVghdb6fqAnsKiw8KToCiGcSgmOdFsCx7TWJ7TW6VhuXRaVp40Grt0Z1xM4SyFkTlcI4VSKcyBNKTUQGJjrqcXZd74ByzXEz+Ralwy0yrOJV4EtSqmhQEWgfWHvKUVXCOFUivM14Ny3FvuXegHvaa1nKaUeAj5USjXRWt/wdmalXnR/XNC5tN/C5qpHzbV3CKXi/Pph9g6hxClHP2nzX3LWvEpCCZ4ylgL45Xrsm/1cbv2BMACt9ZdKqXJAdcDEDcicrhDCqZTgnO5XQH2l1F1KqTJYDpStz9PmNNAu+30bAuWAczfbqEwvCCGcSkmNdLXWmUqpIUA84Aos01ofUUpNBA5ordcDI4AlSqnhWA6qPakLuQeaFF0hhFMpyW+kaa3jgLg8z03I9fMPwCPF2aYUXSGEc3Hw6W4pukIIp+LoXwOWoiuEcCpywRshhLAlx665UnSFEM5FRrpCCGFDUnSFEMKGpOgKIYQNyS3YhRDChmSkK4QQNiRFVwghbMjBa64UXSGEc5GRrhBC2JCLHEgTQgjbcfCB7u1zEfO9e3bRM6Yj3SLD+ODdJfnWp6enM37UCLpFhjHg8Z78dvb6Bd6P/XyUp5/oTZ+ukTzWPZqrV6/aMvQiCX7gDr5f8gSHl/bjxW4t8q2v412ZuCld2L/oMeKndaV29Up2iLJge3bvIjoijMjwEJa9k//OJ+np6YwaMZzI8BD69urO2ZRkANLSLvJ0v8d5uEUzpk6aaPWaAU/2JToijB5dounRJZoL58/bJpddO4nsGEpEWDBLlxScy0sjhhERFkyfnt1Iyc4FYOmSt4kICyayYyh7du8C4OSvJ+jeOSpnebhlM5Z/8B4Ab76xgPaBrXPW7dq5wyY5frlnF12jwuncKZT3lxX8WRo7cjidO4XS77EenE2xfJaOHDpIn+4x9OkeQ+/u0SQlbs15zacffUDPLp3o0TmCT5a/b5M8bsTFRRV5sYfbYqRrNpuZOW0S8xYtwdtgoP9jPWgdEMhddf1z2mxYu4rKHh6sXL+ZrfFxLJo3m/9Nm0VmZiavvTyaCa9Pof7dDfgjLQ03N8dK28VFMXdwEB3Hribl90vsntebjfuO89PpCzltpgxow0cJP/LRth8I+I8fE598lP4zN9sxaguz2czU1yfy5pJlGHwM9OnRjYDAIOrVu75v1q7+nMoeHqzftIXNcbHMmz2LabPmULZMWf479HmO/fILx4/9nG/bk6bOoHGTpjbNZfKkiby95F0MBgO9e3SlbWAQ9fyv57Jm1Uo8PDzYuHkrm+JimTt7JjNmzeX4sWNsjotl9fpYTCYjzwzox/rYeO68qy4rVq/L2X5wYBuC2gfnbK/v40/yRL/+Ns1x+pT/sfCtpXgbDDzRpzutAwKpm2t/rV/zOZU9PFm9IZ4tm2NZOG8mk6fPoZ5/fd7/eCVubm78fs5En+4xtG4TyKmTJ1i7eiXvLV+Bm7s7zw9+mkfbtMWvzh02yys3GemWgB8OH8LX14/avn64u5ehfWgHdm1Psmqza3si4RGWuyMHtgvhwFd70Vqzf+8X1Kt/N/XvbgCAp5cXrq6uNs/hZlrc7cPxs2mcTP2DjMwsVu44SsSD9azaNKhTjR3fnQZgx/dniHiorj1CzefwoYP41amDr59l34SGd2B7YoJVm+2JCXSKigagfUgo+/d9idaa8hUqcH+zByhbtowdIs/v8KGD+PndYcmlTBnCOnRke5J1LkmJiURGxQAQHBLK/r2WXLYnJRDWoSNlypTB19cPP787OHzooNVr9+39Ej8/P2rVqm2znPI6cvggvn51cj5LIaEd2Lk90arNju2JdOxk+SwFtQ/lq/2Wz1K58uVzBixX09NzDlj9euIEjZvem7O+2QMtSErYir2U4O16SsVtUXTPnTNi8KmZ87iGt4FzJmOeNiYMPj4AuLm5UbFSZf5IS+PMqZMopRj236d5sndXlr+31KaxF0Wt6pVIPncp53HK75epXc16+uDQiXNEPVIfgKiH/fGoUJaqlcvZNM6CmEzW+8Zg8Mm3b0wmEz7Zbdzc3KhUqTJpaWmFbvvV8WPp0SWaxW8topA7oJQIk9GIT02fnMfeBgNGY95cjNa5VK5MWtpFjEZjTv8DMPgYMOV57eZNsYR1iLB67tOPP6JrTCcmvDyGP//4o6RTyuecyWQVp7ehgM9Srn16bX/9kb2/Dh/6nh6dI+jdNYpRL7+Cm5sb9fzr8903X5OWdpF//v6bPbt3YjSmlnouN6JU0Rd7+NdFVynVryQDKS1ms5mD333Dq5Om89bSD9mRlMCBfXvtHVaxjXlnJ62b1ubLhX1o3dSXlN8vYc4q/UJkL5OnzWTlmg0s+2A53359gI3r19k7pFuSkZ7OjqREQkLDcp7r3qMXGzdvZcWqddSo4c3MGVPtGGHRNGn6Hz5bvZH3PlrB+0uXcPXqVe6qW4/H+w3guUEDeG7w09x9TwNc7XghcRcXlyIvdonvFl772o1WKKUGKqUOKKUOFDRRX1w1ahgwpv6W8/icyUgNb0OeNt4YUy2/XTMzM7ly+RKeXl7UMBi4r9kDeFWpQrny5Xn40dYc/emHW46pJJ39/TK+NSrnPK5dvRIp5y9btfntwhV6vr6Rh4Z8xCvv7wHgjyv2PyDo7W29b4zG1Hz7xtvbm9TsNpmZmVy+fAkvL6+bb9dg2UbFipUI7xjBkcMHb9q+JHgbDKT+dn2EZjIaMRjy5mKwzuXSJby8qmAwGHL6H4Ax1ZiTA8Du3Ttp0Kgx1apXz3muWvXquLq64uLiQueu3Th86FBppZajhre3VZwmYwGfpVz79Nr+8syzv+6qW4/yFSpw/NgvAETFdOWDT1axeNlyPCp7UueOO0s1j5u5rUe6SqmDN1gOAYYbvU5rvVhr3Vxr3fyJp56+5SAbNm5C8pnTnE1JJiMjnW3xcTwaEGjVpnVAIJs2WkZDSQlbeKBFK5RStHroEY4f+4V//v6bzMxMvv36AHfWrVfQ29jNgZ9T8a9VhTsMHri7udAt4B5i956walPNo1xOJ3mpRwve33LEDpHm17hJU06fPkVKsmXfxG+Ko21gkFWbgMAgNqxbC8C2LfG0aPXgTefTMjMzuXjxIgAZGRns3LGdev53l1oO11hyOUly8hky0tPZHBdLQJ5c2gYGsX7dGgC2bomnZXYuAYFBbI6LJT09neTkM5w+fZImTe/Ned2muFjCO3S02ta5c6acnxO3bcO/fv1SzM6iUeOmnDl9ipTsz9KW+Dha5/kstQkIJHbDuuy44mnewpJjSkoymZmZAPx2NoVTJ0/kzE9fuGA5uyT1t7MkJW4lNNx6GsWWHH1Ot7DD+AYgFLiY53kFfFEqERXAzc2NF0aNY/jggZizsoiIjKFuPX+WvLmABo0a0zogiIjoLkwcP5pukWF4eHoyccpMADw8POnZ5wn69+0BSvHwI615pHWArUIvEnOWZvibiWx4vTOuror3txzhx9PnGd/3Ib752UjsvhO0udePiU8+gtaw+3AywxYlFb5hG3Bzc2PU2PH895n+ZJmziIrpQj3/+ixaOJ9GjZvQNjCI6M5deXnMSCLDQ/Dw9GTqjNk5r+8QEsSVy1fIyMggKTGBRYuXUqtmLQY/05/MjEzMWVm0evAhOnftZpNcxoybwKCBA8jKMhMd0wV///q8sWAejRs3oW1QO2K6dGXc6JeICAvGw9OT6TPnAODvX5+QsHBiIjvg6urK2Jcn5Byw/euvv9j7xReMf8X6tLg5s2Zw9KefUApq1arN+Fcn5oupNHJ8afTLPDdoAFlZWXSK6kw9//q8vWg+DRs1oU3bICJjuvLKuFF07hSKh4cnk6bNAuD7b7/m/WVLcHNzx8VFMXLMBLyqVAFg1Ijn+fOPNFzd3HhpzHgqe3iUei434uhnL6ibHaBQSi0F3tVa7y5g3cda696FvcH5K5lON/Ho23WBvUMoFefXD7N3CCXOxdE/gf/S1Ywse4dQKjzL3/rJsw/8L6nINefr8YE27yA3HelqrW94AmFRCq4QQtiao/+edaxvCQghxC2Say8IIYQNyVXGhBDChhy85krRFUI4FxnpCiGEDTl4zZWiK4RwLnIgTQghbEimF4QQwoak6AohhA05eM2VoiuEcC4y0hVCCBty8JorRVcI4Vwc/eyF2+J2PUIIUVQuShV5KYxSKkwpdVQpdUwpNfoGbborpX5QSh1RSn1c2DZlpCuEcColNb2glHIF3gCCgWTgK6XUeq31D7na1AfGAI9orS8qpbwL266MdIUQTqUE7xzREjimtT6htU4HPgWi8rR5GnhDa30RQGttohBSdIUQTsVFFX3JfT/H7GVgrk3VBs7kepyc/VxudwN3K6X2KKX2KqXCKESpTy+Ud3ct7bewuQvrh9s7hFJRtevb9g6hxJ3/fGDhjW5DDn6syK6KcyBNa70YWHwLb+cG1AfaAr7ATqVUU6112g3ju4U3E0IIh6OK8a8QKYBfrse+2c/llgys11pnaK1/BX7GUoRvSIquEMKpFGd6oRBfAfWVUncppcoAPYH1edqsxTLKRSlVHct0wwluQs5eEEI4lZL6RprWOlMpNQSIB1yBZVrrI0qpicABrfX67HUhSqkfADPwktb6/M22K0VXCOFUSvIbaVrrOCAuz3MTcv2sgReylyKRoiuEcCpF+dKDPUnRFUI4FUf/GrAUXSGEU3Hwga4UXSGEc5HpBSGEsCHHLrlSdIUQTkYuYi6EEDbk4MfRpOgKIZyLnL0ghBA2JNMLQghhQw4+0JWiK4RwLjLSFUIIG3LskitFVwjhZFwdfH7BYa+nu2f3LqI7hRHZIYRl7+S/sHt6ejqjXhxOZIcQ+vbuztmUZADS0i7y9FOP83DLZkydNDGn/ZUrl+nRNTpnCWz9IDOmTbZZPtfs2b2TqIhQOoUH3zCvkSOG0Sk8mMd6dSMlV14D+vXloRb3MyVXXgAL5s0htF0AD7W43yY5FCb4fj++X9SDw2/15MUu9+Vb71e9Eptfj+DLOV3YP68roQ9YrhPdM8CfvXO65CxX1gzk3ruq2Tj66/bs3kV0RBiR4TfpgyOGExkeQt9eefpgv8d5uIV1HwQY8GRfoiPC6NElmh5dorlw/qZXASwVX+zZRefIcKIjQnlv6ZJ869PT0xnz0nCiI0J5ok8PzqZYrtt9+NBBenePoXf3GHp1iyYpYWvOa16bMI7gto/QvXMnm+VxIyV4j7RS4ZBF12w2M3XSRBYuWsKqdRvZvCmW48ePWbVZu/pzKnt4sD5uC336PsG8ObMAKFumLP8d8jzDXxxp1b5ixUp89vnanKVmzVoEtQu2WU5gyWvK6xN54813WL0+ls1xG/PltWb1Sjw8PNiwaSuP9X2SebNnApa8Bg99nhfy5AUQ0DaQ5Z+utEkOhXFxUcx95hGiXovj/iEr6NbanwZ+XlZtRnVvxqrdJ3ho+Coen7mNec+0BuDTHcd4cPgqHhy+iv5zkzhp/JODv9q+KEF2H3x9IgvfXMKq9RvZHHeTPrgpuw/OztUHh+bvg9dMmjqDz1at5bNVa6lazba/VMxmM9Mm/4/5ixazcs0G4jfHciJPXuvWfE5lD0/Wboyn92OPs2CupQ/6+9fng49X8vGKNSxYtJjJ/3uVzMxMADpFRbPgzVu5603JUaroiz0UWnSVUg2UUu2UUpXyPF/oDdj+rcOHDuJXpw6+fn64u5chNLwD25MSrNpsT0qgU2Q0AO2DQ9m/70u01pSvUIH7mz1A2TJlbrj9Uyd/5cKFCzR7oHlppVAgS1535MqrI9sT8+SVmEinqBgA2ofkzas5ZcqWzbfde/9zHzVqFHrnZ5toUd+b46l/ctJ4iYzMLFbuOkZEyzut2mit8ajgDoBnhbL8dvFKvu10b+3Pyt3HbRFygQrsg/n2VQKdoqKBgvbVA5Qte+M+aC9HDh/Ez68Ovr6WvELCOrBje6JVmx1JiUREWm562y44lP3796K1plz58ri5WWYkr15NtxopNnugBR4eXjbL42ZclCryYpf4brZSKfUcsA4YChxWSuW+/XCp/W1uMhkx+NTMeWww+HDOaMzTxoRPdhs3NzcqVapMWlpakba/eVMcIWHhNv/zwmQy4uPjk/PYYDBgMuXNy1hAXhdtGuetqFWtAsm/X855nHL+CrWrVbRqM+nTr+kZUJ9jS/uwZkI4Lyzek287XR+ty4qdx/I9bysF9sF8++rf9cFXx4+lR5doFr+1CMs1sG3HZDJhyNUHvb0NmPJ9tq7nfi2vP7LzOnzwe7rHRNCzaxRjXn4lpwg7ktt9pPs08IDWOhrLfYDGK6Wez153w5Bz39a4oLkwe4vfHEdYeEd7h/H/VvfW9Vie+DP+/T8iZuImlg4PsvoAtLjbm7+uZvLD6dvnl01RTZ42k5VrNrDsg+V8+/UBNq5fZ++QiqXJvf9hxZqNfPDxCt5duoSrV6/aO6R8bvc5XRet9WUArfVJLIU3XCk1m5sUXa31Yq11c61186cGFP8W2N7eBoypv+U8NhpTqWEw5GnjTWp2m8zMTC5fvoSXl1eh2z569CfM5kwaNW5S7Lhulbe3gdTU1JzHRqMRb++8eRkKyKuKTeO8FWfP/4Vv9eszUbWrVSTlvPX0wRPBDVi1xzJ1sO+okXLurlT3KJezvlvreqzYZb+pBbhBH8y3r4rfB72z+3HFipUI7xjBkcMHSzbwQnh7e2PM1QdNJmNOTNfbXM/9Wl6eefK6q249KlSowPFjv5R6zMXlqlSRF3sorOgalVL3XXuQXYAjgOpA09IKqnGTppw+dYqU5GQyMtKJ3xRH27ZBVm0C2gaxYf1aALZtjadFyweL9Jtrc1ys3Ua5jZs05fTpk6Qkn8nOK5aAwDx5BQaxYd0aALZtiadFq6Ll5SgO/GLCv6Ynd3hXxt3NhW6t/Yndf8qqzZlzl2l7b20A7vH1olwZV8798Q9g+ZOvyyP1WLnLflMLcG1f5emDBe6rtUDR9lVmZiYXL1pG7xkZGezcsZ16/neXWg4FadS4KWdy5bVlcxxtAgKt2rRpG5gzAk/I9dlKSU7OOXD229kUTp48Qa1atW0af1GU4N2AS4W62ZySUsoXyNRapxaw7hGtdf7JuDz+Sv93k1a7du5g5vTJZJmziIrpwoCBz7Jo4XwaNW5C28Agrl69ystjRnL0px/x8PRk6vTZ+PpZTj3qEBrElctXyMjIoHLlyixavJR69fwBiAhrz4JFi7mrbt1/ExZwa9942bVzBzOmTSbLbCYqpgtPPzOIRQvnZefVjqtXrzJuzEsc/dGS17QZc3LyCg8J4srly5a8PCrz5uJl1Kvnz5xZ09kUt5FzJhM1vL2J6dyNQYOHFju2ql3f/td55Rb6gB8z+j+Mq4vi/YSjTF/5LeN7N+ebY+eI3X+KBn5eLBocQMVy7mitGff+PhK+s5xu1bpJTV5/vBUBI9eWSCznPy/+X1rX7Nq5g5nTcvXBZ27QB7P31dQZufpgSK4+6GHpg7Vq1qL/k4+RmZGJOSuLVg8+xIiRo3F1dS12bGbzv58L3r1rB7OnT8GclUVkdGf6P/0sb70xn4aNmxDQ1pLXhHGjLJ8tD08mT5+Fr68fsRvW8f6yJbi5u6OU4uln/kvboPYAjB01gq8P7CctLY1qVasxcNAQojt3LXZslcvdeil8Yf1PRf7PmR3ZwOal96ZFtyT826LryG6nkWdxlFTRdSS3UnQd2a0UXUdWEkV3xIajRf7PmdXpHpt/mB3v0KMQQtwCB/9CmhRdIYRzcfQ/RKXoCiGcipuDV10pukIIp+LgNVeKrhDCucgt2IUQwoYcvOZK0RVCOBc5e0EIIWzI0S9iLkVXCOFUHLzmStEVQjgX5eB3SZOiK4RwKjLSFUIIG5KiK4QQNuToF6RyyBtTCiHEv+XqUvSlMEqpMKXUUaXUMaXU6Ju066KU0kqpQm+8KCNdIYRTKalvpCmlXIE3gGAgGfhKKbVea/1DnnaVgeeBfUWKr0SiE0IIB1GCd45oCRzTWp/QWqcDnwJRBbT7HzAN+Kco8ZX+SNexp1f+FY1zXkDa9NnT9g6hxFVrWfw7aNwOzu9bYO8QHFZxBrpKqYFA7ivdL9ZaX7ubbm3gTK51yUCrPK9vBvhprWOVUi8V5T1lekEI4VRcijHSyy6w/+qW5UopF2A28GRxXidFVwjhVErw5IUUwC/XY9/s566pDDQBtmefMeEDrFdKRWqtD9xoo1J0hRBOxa3kTtT9CqivlLoLS7HtCfS+tlJr/QeWO6MDoJTaDrx4s4ILciBNCOFklCr6cjNa60xgCBAP/Ais0FofUUpNVEpF/tv4ZKQrhHAqJXkRc611HBCX57kJN2jbtijblKIrhHAqDv6FNCm6Qgjn4uhzplJ0hRBORe6RJoQQNiRFVwghbMixS64UXSGEk3Hwga4UXSGEc3H06+lK0RVCOBU5e0EIIWxIDqQJIYQNyfSCEELYkEwvCCGEDTn6SNdhfyns2b2L6IgwIsNDWPZO/msMp6enM2rEcCLDQ+jbqztnU5IBSEu7yNP9HufhFs2YOmliTvu///6boYOeIaZTOF2iIpg3Z5bNcsmtpPPK7fkhg+ga3alU47+RL/bsonNkONERoby3dEm+9enp6Yx5aTjREaE80acHZ1MslyU9fOggvbvH0Lt7DL26RZOUsNXqdWazmd7dOzNsyLM2yeNGgh9uyPdrxnN43Su82C843/o6NasQ99ZQ9n82hvglz1Pb2ytn3evPRXFg5VgOrBxL15BmNoy6YHt27yK6UxiRHW7SB18cTmSHEPr2ztMHn3qch1vm74Ob4jbSLaYT3TtHMvjZAVy8eNEmuRREFWOxB4csumazmamvT2Thm0tYtX4jm+NiOX78mFWbtas/p7KHB+s3baFP3yeYN9tSRMuWKct/hz7P8BdH5tvu4/36sWbDJj79fDXff/sNu3fttEk+15RWXgAJW7dQoUKFUs+hIGazmWmT/8f8RYtZuWYD8ZtjOZEnr3VrPqeyhydrN8bT+7HHWTB3JgD+/vX54OOVfLxiDQsWLWby/14lMzMz53WffPQhd9Wta9N88nJxUcwd3Z2oIYu4v8vrdAt7gAZ1fazaTBkew0ex+2nZYwqTF29i4lDLlf/CHm3MfQ39aNVzKm36zmTY4+2oXLGcPdIAsvvgpIksXLSEVes2snnTTfpgXHYfnJOrDw7J3wczMzOZMW0yi5d9wIrV66l/9z189slym+WUl6tSRV7swSGL7uFDB/GrUwdfPz/c3csQGt6B7YkJVm22JybQKSoagPYhoezf9yVaa8pXqMD9zR6gbNkyVu3Lly9Pi5YPAuDuXoYGDRthMqbaJJ9rSiMvgL/+usLyD95jwDODbJFGPkcOH8TPrw6+vpa8QsI6sGN7olWbHUmJRERa7unXLjiU/fv3orWmXPnyuLlZZrmuXk23+tPQaExlz64dRMd0tV0yBWjR5E6On/mdkynnycg0szL+GyLa3mvVpkHdmuzYfxSAHV/9TETbpgA0rOvD7m+OYTZn8dc/6Rz6JYWQhxvaPIdrCuyDSXn6YFICnSKjAWgfXEAfLGPdB7XWaK35+++/0Fpz+fJlatTwtlVK+ZTU9XRLS6FFVynVUinVIvvnRkqpF5RSHUozKJPJiMGnZs5jg8GHcyZjnjYmfLLbuLm5UalSZdLS0oq0/Ut//snOHUm0bPVQicVcFKWV16IF8+n7RD/Kl7PPCMpkMmHwuT7y8/Y2YDLmzet67tfy+iM7r8MHv6d7TAQ9u0Yx5uVXcorwrOlTeG74iygX+44Nanl7kmy8/udyivEitWt4WrU59HMKUUH3ARAV9B88KpWnqmdFDv5sKbLly7lTzasiAc3vxtenii3Dt1JgH8y3r4rXB93d3Rn78it07xxJSFAbThw/TnRn+/2iVMX4Zw837c1KqVeA+cCbSqkpwEKgIjBaKTXOBvGVuMzMTEaPHEGvPn3x9fMr/AUO7uhPP3LmzGmC2uefZ7xdNLn3P6xYs5EPPl7Bu0uXcPXqVXbtSKJq1ao0bNTY3uEVyZg5a2j9gD9ffjKK1g/4k2K8iNmcRcLen9i8+weS3hvB+1P6se/gr5jNWfYOt0RlZGTw+YpP+WTlGrYk7uTuu+8ucK7YVm73kW5X4BGgDTAYiNZa/w8IBXrc6EVKqYFKqQNKqQP/5j/f29uAMfW3nMdGYyo1vA152niTmt0mMzOTy5cv4eXlVei2X391AnXq3EGfvk8UO65bVRp5ff/dd/xw5DAdQoLo93gfTp08yYAn+5ZK/Dfi7e2NMfX6VI3JZMTbkDev67lfy8szT1531a1HhQoVOH7sF77/7lt2bk+iU3g7xo0awVdf7WP8mILns0vbWdMf+Bquj05rG6qQcu4Pqza/nfuDni++w0O9pvHKwg0A/HH5bwCmL43nwZ5TiRi0EKUUv5w22S74PArsg/n2VfH64M9HfwLAz68OSimCQ8P5/rtvSz74InJBFXmxT3w3l6m1Nmut/wKOa63/BNBa/w3c8Ne11nqx1rq51rr5UwMG3qjZDTVu0pTTp0+RkpxMRkY68ZviaBsYZNUmIDCIDevWArBtSzwtWj1Y6Kkib8yfy6XLl3hp9Nhix1QSSiOv7j17sTVpF3FbEnn3g4+44847eee9D0szjXwaNW7KmVx5bdkcR5uAQKs2bdoGsnH9OgAStsbToqUlr5Tk5JwDZ7+dTeHkyRPUqlWbIc+/QNzW7WzYlMCkabNo0aIV/5sy3aZ5XXPgyCn869TgjlrVcHdzpVtoM2K3H7RqU82rYs5+eumpUN5ftxewHISr6lkRgCb1a9Gkfi22ffmTbRPIpXGTppw+lacPts3TB9sGsWH9WgC25dpXN1LD25sTx49z4cIFAPZ++YVdD346+ki3sPN005VSFbKL7gPXnlRKeXKTonvLQbm5MWrseP77TH+yzFlExXShnn99Fi2cT6PGTWgbGER05668PGYkkeEheHh6MnXG7JzXdwgJ4srlK2RkZJCUmMCixUupVLES7yx+i7vuqkuvbp0B6NGrD527diutNGySV716/jaL/0bc3Nx4aczLDB00AHNWFpHRnannX5+33phPw8ZNCGgbRFRMVyaMG0V0RCgeHp5Mnm45Iv7dt1/z/rIluLm7o5Ri9NgJeFWx35xnQczmLIZPW8GGRYNxdVG8v24vP55IZfygjnzzw2lidxyiTfP6TBwaidaw+5tjDJuyAgB3N1e2LRsGwKXL//DUuPftOr2Q0wefLUIf7JDdB6fn6oOhBffBgYMGM+DJx3Bzc6NmrVq89voUu+Xo6F8DVlrrG69UqqzW+moBz1cHamqtDxX2Bn9l3OQNhEMxm51vV3k/9Jy9QygV5/ctsHcIpaJCmVuvmAk//V7kjtyuQXWbV+ibjnQLKrjZz/8O/F4qEQkhxC2w11kJRSVfAxZCOBUHn12QoiuEcC4y0hVCCBtyceyaK0VXCOFcHP3sBSm6Qgin4tglV4quEMLJyEhXCCFsyLFLrhRdIYSzcfCqK0VXCOFUZHpBCCFsyLFLrhRdIYSzcfCqK0VXCOFU5BtpQghhQw4+peuYN6YUQoh/qyRvwa6UClNKHVVKHVNKjS5g/QtKqR+UUgeVUglKqTsK26YUXSGEU1FKFXkpZDuuwBtAONAI6KWUapSn2bdAc631vcDnQKG3N5GiK4RwKiV4u56WwDGt9QmtdTrwKRCVu4HWOin7zjoAewHfwjZa6nO6l/7OLO23sLlK5ZxzKryw3/y3owv7F9o7hFJRrecye4dQKv5a9dQtb6M4vVgpNRDIfSPHxVrra3fTrQ2cybUuGWh1k831BzYV9p7OWT2EEP9/FaPqZhfYW75fvFLqMaA5EFBYWym6QginUoKnjKUAfrke+2Y/Z/1+SrUHxgEBN7rFWW4ypyuEcColOKf7FVBfKXWXUqoM0BNYb/1e6n7gbSBSa20qSnwy0hVCOJWSOjShtc5USg0B4gFXYJnW+ohSaiJwQGu9HpgBVAJWZh8TOa21jrzZdqXoCiGcSkl+I01rHQfE5XluQq6f2xd3m1J0hRBOxdFPwpGiK4RwKg5ec6XoCiGcjINXXSm6QginIhcxF0IIG3LskitFVwjhbBy86krRFUI4FbmIuRBC2JCDT+lK0RVCOBcHr7lSdIUQzsXRL1EqRVcI4VQcvOZK0RVCOBcHr7mOW3T3fbGb+bOmkpVlpmNUFx57coDV+vT0dCa9Moaff/oBD08vXp08k5q1arNl00Y+/fDdnHbHj/3MOx+upLavH0Oefjzn+XMmI8HhETw3It+95mxmz+5dzJw2CbM5i5jOXek3YKDV+vT0dMaPHcWPPxzBy8uLqTNmU6u2L3u/2MP8ubPIzMjAzd2dYSNG0rLVg3bKwuKLa7lkZRHduSv9+ufPZcI4Sy6enrly+XIPC+bOIiMjA3d3d55/IX8uw4cOIiU5mRVrNtgyJSt7du9k+tRJZJmziOnSjacK2Fcvjxlpyc/Li2kz51C7ti9paRd5cfhzHDl8mMjoGMaMm3CDd7CP4PtqM+OpB3F1UbyX8DOz1hy0Wu9bvSJLhrbBq0IZXFwVE5YfIP6bZACa3FGFBc88QuUK7mRlaVqP2sDVDLM90rDm4FXXIYuu2WxmzvTXmb1wCTUMPgx8ogePtgnkzrr1ctrErltNZQ8PPlmziYQtcby1YDavTZlFSHgEIeERgKXgjnvxOerf0wCAZR+vynn9gL7daRNY7AsElRiz2cy0SRNZtHgZBh8Dj/XsRkBgEHXr+ee0Wbv6czw8PFgft4X4TbHMmzOLaTPn4FWlCvMWvkkNbwPHfvmZwc8OID5hp11zmTo5OxeDgb69uhHQtuBc1sVacpk/dxZTZ8zBy6sKcxdcz2XIoAFs3nY9l8RtWyhfoYI90sphNpuZ8vpE3lryLgYfA316dCUgMIh6ufJbs3olHh4ebNi0lc1xscybPZPps+ZStkxZBg99nmO//MKxY7/YMYv8XFwUc55+iIiJ8aScv8KuaZHEfnWan5LTctqM7nofq7/4lSXxP9HA14s144JpOGglri6Kpc8HMGDeTg6dukDVSmXJMGfZL5lcHP2UsWJfxFwp9UFpBJLbj0cOUduvDrV8/XB3d6ddcDi7dyRatdm9M5GwjpZ7xAUEhfDNV/vQWlu1SYiPo11IeL7tnzl1kosXzvOf+x8ovSQKcfjQQXzr1MHXzw939zKEhndge1KCVZvtSQlEREYD0C44lK/2fYnWmgYNG1HD2wBAPf/6XP3nKunp6bZOIceRwwfxq1MHX19LLiFh+XPZsd06l/1FyOWvv66w/MP3GDBwkE3zyevwoYP41bkj177qyPbEPPsqMZFOUTEAtA+5nl/5ChW4v1lzypQta4/Qb6q5f3WOp/7JSeMlMjKz+Hz3CSJa1LFqo7Wmcnl3ADwquPPbBcs9GNvfV5vDJy9w6NQFAC5cvkpWlvXnz15K8CLmpeKmRVcptT7PsgHofO1xaQX1+zkT3gafnMc1DAbOnbO+KPvvputt3NzcqFipEn/8kWbVJnHrZtqFdMi3/YQtmwgKDrPrUc5zJiM+PjVzHnsbfDAZjXnamHLauLm5UalSZdLS0qzaJGyNp0HDRpQpU6bUY74Rk9GIwXA9F4PBh3OmPLkYTTltiprLmwvn89jj/ShXrlzpJlAIk8mIj8/1/mgwGDDlyc+Ua39ez++iTeMsrlpVK5Ly+5WcxykXrlCrmvVfFZM++5aeberxy+IerBkXwoilewHwr+mBBtaND+GLGZEMj2pqy9BvykUVfbFLfIWs9wX+BGYDs7KXS7l+LpBSaqBS6oBS6sCH775TUrEWyw+HD1K2XHnq+tfPty5h6ybah+Yvxreb48d+Yf6cWYx75TV7h3LLjh/7hflzZzF2giWXoz/9SPKZ0wS1C7ZzZP+/dWtdl+VJx6g/8DNiJm3hnefaoBS4ubrwcAMDT83dQbtxsUS2uoO2TWsWvkGbUMVYbK+wotsc+BrLTdf+0FpvB/7WWu/QWu+40Yu01ou11s211s379htwo2Y3VL2GNyZjas7jc0YjNWp4W7fxvt4mMzOTK5cv4+nplbM+Ycsm2ofmn1o49vNPmM1m7mnYuNhxlaQa3gZSU3/LeWwypuJtMORp453TJjMzk8uXL+Hl5QWAMTWVEcOGMHHyNPz8rP8ktDVvgwGj8XouRmNqzpTBNTUM3jltCsrlxeFDmDjpei4Hv/+OH344TERYEP2f6MOpUycZ+FRf2ySUh7e3gdTU6/3RaDTinSc/71z783p+VWwaZ3GdvXCF2tUr5jyuXbUiZ8//ZdXmiXZ3s+qLXwHY//M5ypVxo3rlcqScv8LuH1I5f+kqf6ebif/mDPfVrWbT+G/ktp5e0Fpnaa3nAP2AcUqphdjg4FuDRk1IPn2asynJZGRkkLB1E4+0CbRq80jrQDbHrgNgR+IWmrVolTNdkJWVRdK2eNoF5y+62+I30b6AeV5ba9ykKWdOnSIlOZmMjHTiN8UR0DbIqk1A2yA2rl8LWP70btHyQZRSXPrzT54b/AxDh43gvvub2SF6a40aW+eyZXPxcnl+yDMMfd46l249ehGfsIuNmxNZ+v5H3HHHnSxe9qEt08rRuElTTp8+SUrymex9FUtAYJ78AoPYsG4NANu2xNOi1YMOf5L+18d+x7+mJ3d4V8LdzYWuj9Yl9sBpqzbJ564QeK9lBHtPbU/Kubty7s9/2PZdCk3uqEL5Mq64uigebVyTn86k2SGL/Bx7nFvEAqq1Tga6KaU6YpluKFVubm4MGzmWF597hiyzmQ6RMdxVz5+lby3knoaNeTQgkI5RnZn0yhh6xYRT2cOTVyfNyHn9998ewNvgQy1fv3zbTtoWz/R5i0o7hUK5ubkxaux4Bj/bnyxzFpExXajnX583F86nUeMmBAQGEd25K+PHjCSyQwienp5MmT4bgM8++YgzZ06z5K1FLHnLksuit5dStZp9Rhpubm6MHDueIYP6YzZnERWdncsb82nUyJJLVExXxo8dSVRHSy6Tr+Xy6UecOX2aJW8vYsnbllzeeMt+uRTEzc2N0WMnMOiZAWSZzUTFdMHfvz6LFs6jUeMmtA1sR0znrowb8xKdwoPx8PRk2ow5Oa8PDwniyuXLZGRkkJS4jTcXL7M688FezFmaF975kvXjQ3F1UXyQ+As/nkljfM/7+ebY78QeOMPo9/fzxqBHGBLRBLRm4ELLmSVpV9KZv+EIu6ZHojXEf3OGzdmnktmbg/+uQ+U94l/SjH9mOMYhzRJUqZxDnml3y0q5K9iFq72OlpSyaj2X2TuEUvHXqqdueYelFqPm+Hi427yDOGf1EEL8v+Xov2al6AohnIqjTy9I0RVCOBVH/0aaFF0hhHNx7JorRVcI4VwcvOZK0RVCOBe5BbsQQtiQg9fc4l9lTAghxL8nI10hhFNx9JGuFF0hhFORU8aEEMKGZKQrhBA2JEVXCCFsSKYXhBDChmSkK4QQNuTgNVeKrhDCyTh41ZWiK4RwKo7+NeBSv3OELSmlBmqtF9s7jpLmjHk5Y07gnHk5Y0725GxfAx5o7wBKiTPm5Yw5gXPm5Yw52Y2zFV0hhHBoUnSFEMKGnK3oOuu8kzPm5Yw5gXPm5Yw52Y1THUgTQghH52wjXSGEcGhSdIUQwoacougqpcKUUkeVUseUUqPtHU9JUEotU0qZlFKH7R1LSVJK+SmlkpRSPyiljiilnrd3TLdKKVVOKbVfKfV9dk6v2TumkqSUclVKfauU2mjvWJzBbV90lVKuwBtAONAI6KWUamTfqErEe0CYvYMoBZnACK11I+BBYLAT7K+rQJDW+j/AfUCYUupB+4ZUop4HfrR3EM7iti+6QEvgmNb6hNY6HfgUiLJzTLdMa70TuGDvOEqa1vo3rfU32T9fwvJhrm3fqG6Ntric/dA9e3GKI9RKKV+gI/COvWNxFs5QdGsDZ3I9TuY2/xD/f6GUuhO4H9hn51BuWfaf4N8BJmCr1vq2zynbXGAkkGXnOJyGMxRdcRtSSlUCVgHDtNZ/2jueW6W1Nmut7wN8gZZKqSZ2DumWKaUiAJPW+mt7x+JMnKHopgB+uR77Zj8nHJRSyh1Lwf1Ia73a3vGUJK11GpCEc8zHPwJEKqVOYpm2C1JKLbdvSLc/Zyi6XwH1lVJ3KaXKAD2B9XaOSdyAUkoBS4Eftdaz7R1PSVBK1VBKeWX/XB4IBn6ya1AlQGs9Rmvtq7W+E8vnKlFr/Zidw7rt3fZFV2udCQwB4rEclFmhtT5i36hunVLqE+BL4B6lVLJSqr+9YyohjwB9sYyavsteOtg7qFtUE0hSSh3EMgjYqrWW06tEgeRrwEIIYUO3/UhXCCFuJ1J0hRDChqToCiGEDUnRFUIIG5KiK4QQNiRFVwghbEiKrhBC2ND/AbFWMoqyCJC5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm, annot=True, cmap=\"Blues\")\n",
    "plt.savefig(f\"images/{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81      1492\n",
      "           1       0.93      0.90      0.91      3476\n",
      "           2       0.91      0.87      0.89      3017\n",
      "           3       0.96      0.99      0.97     15462\n",
      "           4       0.87      0.86      0.86      2890\n",
      "\n",
      "    accuracy                           0.94     26337\n",
      "   macro avg       0.90      0.88      0.89     26337\n",
      "weighted avg       0.94      0.94      0.94     26337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train[\"label\"], test[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9362114136006379\n"
     ]
    }
   ],
   "source": [
    "print(get_score(train[\"label\"], test[\"label\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
